# Music Mood Classification

## Project Motivation
The project aimed to investigate the detection of mood and music using audio features. I'm sure that I could relate to not being able to find the perfect song that fits my emotional situation. It was rather difficult to simply choose music based on how I felt. That's why I decided to investigate the detection of mood from music. Moreover, this model could potentially be used for creating recommendations based on how the users feel.

## Dataset
Two types of dataset were used:
1. Tag Dataset: I used a musical dataset with tags as labels for training the model. It consisted of a training and test dataset, each containing song IDs and tags. These tags were generated by experts, and in total, there were about 94 tags.
2. Audio Features: It's a dataset of the respective songs from Tag Dataset with its audio feature. This dataset has a high dimensionality of features. There were two main types of features: BLF and PS09. BLF features had a total of six features within themselves, and PSO9 had a total of 12. Each of these features was segmented into blocks thus each features are in the form of vectors. For instance, feature one of BLF had a feature vector where the value v1 represented the value obtained from block 1 when segmenting the audio.
![Audio Feature Original](/plots/audiofeature_ori.png)

## Data Preprocessing
The problem with the tag dataset was that the tag list consisted of a mixture of genres and moods. Another challenge was having too many tags for each song, representing a mixture of mood and genres. Since I treated this as a multi-class problem, the dataset should contain only one label for each song. To overcome the challenge of having musical tags that were a mixture of mood and genre, I created a crowdsourcing survey form to determine whether a tag could be identified as a mood or not. I considered a tag to be a mood if it had the majority of votes identifying it as a mood.
The crowdsourcing helped in selecting moods, but there was still a challenge of having too many mood tags for each song. To address this, I computed the semantic similarity between each mood tag and applied hierarchical clustering. ![Moods Hierarchy](/plots/moods_similarity_hierarchy.png) I manually grouped the moods further, and this led to some songs having more than one label even after choosing the most frequently occurring label for each zone. To resolve this, I created a priority system for cases with multiple different labels. ![Grouped Moods](/plots/grouped_moods.png) 

## Feature Engineering
Another challenge was dealing with the high dimensionality of the audio feature dataset. My approaches to tackle this were as follows:
- Naive Approach: I simply took the median of each vector for each song and trained a KNN on it. ![Audio Feature Median](/plots/audiofeature_median.png)
- Smart Approach: I treated each block as a single feature, so the value from block 1 was considered a single feature. ![Audio Feature Encoded](/plots/audiofeature_encoded.png) For the smart approach, I experimented with three different strategies:
  - Using all these features as they were for training
  - Performing PCA to decompose all these features
  - Using XGBoost to only extract the important features

## Machine Learning Model
Since the main idea of KNN is to put observations that are close to each other in the same class, I made use of it in this project as I wanted similar emotions to be grouped together. I aimed to use as few features as possible to make the model less complex, and XGBoost had the ability to choose the most important features. XGBoost also internally regularized the data, thus avoiding overfitting of the model. Hence, I also tried XGBoost for this project.

## Result
This gave me the following results:
- The naive approach provided an accuracy of 16%, and the overall classification was not satisfactory.
- For the smart approach, it resulted in: 
  - 19% accuracy for using all the features approach.
  - 20% accuracy for the PCA and XGBoost approach.
  - The overall performance was better than the naive approach, and XGBoost was able to correctly classify "n strong" better than the other two smart approaches. However, there were still misclassifications, particularly towards the "love" and "n strong" classes.

![Confusion Matric of Naive Approach](/plots/cm_naive.png)
![Confusion Matric of All Features Approach](/plots/cm_all.png)
![Confusion Matric of PCA Approach](/plots/cm_pca.png)
![Confusion Matric of XGB Approach](/plots/cm_xgb.png)

Based on the results, it can be seen that unfortunately, it is not logical to say that this model can be used to accurately detect mood for music. This could be due to various reasons:
- The crowdsource responses on the tags may have influenced what tags were considered mood and what types were not. Having more responses might have changed the outcomes.
- The grouping of mood types may not have been done correctly, and prioritizing labels may not have been the best approach. Additional crowd sourcing or alternative grouping methods could have yielded better results.
- The features used may lack information about mood. Exploring different features or utilizing other techniques to extract mood from sound could potentially improve the model's performance.
- It is also possible that the features were not processed correctly. Applying different pre-processing techniques based on domain knowledge of audio features might enhance the model's performance.

Nonetheless, I believe I learned valuable insights from this project regarding the challenges that need to be addressed before training models on mood detection. Treating this as a multi-level problem rather than a multi-class problem could be more helpful in the future.